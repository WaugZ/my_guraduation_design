论文主体的内容根据不同学科有不同的特点，一般应包括以下几个方面：
（1）毕业论文（设计）总体方案或选题的论证；
（2）毕业论文（设计）各部分的设计实现，包括实验数据的获取、数据可行性及有效性的处理与分析、各部分的设计计算等；
（3）对研究内容及成果的客观阐述，包括理论依据、创新见解、创造性成果及其改进与实际应用价值等；
（4）论文主体的所有数据必须真实可靠，凡引用他人观点、方案、资料、数据等，无论曾否发表，无论是纸质或电子版，均应详加注释。自然科学论文应推理正确、结论清晰；人文和社会学科的论文应把握论点正确、论证充分、论据可靠，恰当运用系统分析和比较研究的方法进行模型或方案设计，注重实证研究和案例分析，根据分析结果提出建议和改进措施等。

Reference(or may be refered)
https://en.wikipedia.org/wiki/Convolutional_neural_network
http://ieeexplore.ieee.org/document/726791/?reload=true&arnumber=726791

# 第一章 简介（背景）

我的设计一键建模系统(One Button Modeling System)应用了当下图像处理中较流行的卷积神经网络，使用了爬虫技术作为数据获取的手段，Flask和Jina2框架作为数据展示的平台，它的功能是用户在网页输入一个（一组）关键词，服务器将根据此关键词从京东(www.jd.com)中抓取数据，然后经过聚类，进行自动化的数据清理，利用清理后的数据进行建模；建模完成后，用户可以输入一张图片，服务器将把这张图片经过模型，用置信度的方式告诉用户这张图片与其建模对象是否一致。

最近深度学习技术常常被主流媒体曝光，其智能让许多人惊艳，但苦于技术、算力和数据的门槛，一般人很难近距离的接触这项技术，只能远观，等待着技术成熟，自己能够体验到这项技术。而这个系统可以让对深度学习技术知之甚少甚至完全不了解的人也能够简单地、只需按下一个按钮地创建属于自己的模型，感受深度学习技术的魅力与其强大的图像识别能力。

## 1.1 卷积神经网络

### 1.1.1 卷积神经网络综述

卷积神经网络(convolutional neural network, CNN)是机器学习中一种深层的、人工构造的前馈神经网络，在计算机视觉领域有着出色的效果。卷积神经网络使用多层感知器（multilayer perceptron, MLP）的变体来作为数据预处理，这种处理的特征是空间不变性，因为网络中层与层之间的参数是共享的。
卷积神经网络期初启发于一个生物实验：该实验发现被实验的哺乳动物仅在视野中有明显的边界变化时，最初的视觉神经元会有激活反应，这与图像处理中的卷积类似，同样适用于处理边界信息，卷积神经网络便在这一启发下应运而生。
CNN较于传统的神经网络更适合于处理图像，因为它需要的预处理要少得多，它几乎是端对端的，即输入是原始图片输出是网络的预测结果，网络将自动学习到许多用于识别图像的特征，而传统的方法则需要人工构造这些特征，它不需要人工的先验知识并尽可能避免人为构造特征可能对后续设计的影响，这是CNN作为机器学习方法之一的一大优势。

CNN包括一个输入层、一个输出层和若干隐藏层，隐藏层一般由三种结构组成，它们分别为卷积层(Convolutional layer, conv)、池化层(Pooling layer, pool)、全连接层(Full connected, FC)

在卷积层中，对输入数据进行卷积操作（在数学领域中，应当称为“相关(cross-correlation)”），将输出经过一个激活函数（包括经典的曲线激活函数(Sigmoid)，在深度网络中更有实践意义的tanh函数和ReLu函数）后传入下一层中。每个卷积层神经元仅在相关的区域进行数据操作，也即局部性。传统的全连接前馈神经网络不是一个适合处理图像数据的结构，若要这么做，则需要大量的神经元作为图像的像素点作为输入，而全连接这些神经元则需要大量的参数，例如一张100*100*3的图片作为输入，第二层为96*96*3，全连接这2层神经元需要829470000((96^2*3+1) * (100^2 * 3))个参数，这样不必说深层神经网络结构，就算是浅层结构也需要大量的参数和大量的算力，更重要的是在反向传播中会出现梯度消失或梯度爆炸的情况，导致网络无法训练。而使用具有局部特性的卷积层，则只需要78(（5*5+1）*3 )个参数即可完成两个神经网路层间的数据传递，同时数据处理的局部性更贴近于生物的视觉——接近的像素点更可能有某种联系。

池化层常出现在CNN中，它将多个神经元的输出到单一个神经元中，起到一种类似于压缩的作用。常用最大池化层，即将输入的多个神经元取激活值最大的那个作为输出。池化层的主要作用是压缩数据，加快训练网络的速度，但同时压缩导致的数据失真也被其他一些图像处理的方法所诟病。

全连接层与传统多层感知器（multilayer perceptron, MLP）中层与层之间神经元的联系大体相同。在CNN中，输入从一般先通过若干卷积层和池化层，得到网络自动提取的图像特征值——一个向量，然后经过若干个全连接层，最后连接到输出层，对应不同的任务有多种输出层，如处理多分类的softmax loss，处理实数标签的Euclidean loss等。

### 1.1.2 检测网络

### 1.1.3 识别网络

## 1.2 爬虫技术

## 1.3 flask与jina2

# 第二章 相关研究

# 第三章 实验过程

# 第四章 结论

------------------------------------------------------------------------

# 第一章 背景

## 1.1 研究背景和意义

【*****生物论文】（CS231，Feifei Li）中指出，*****时代的生物爆炸式进化是由于视觉的诞生，计算机视觉这些年的发展也许也可以引起许多计算机技术的高速发展。卷积神经网络(convolutional neural network, CNN)是实现计算机视觉为之有效的技术之一，其应用之一的图像识别的准确率在日益上升，从2010年开始每年举办的The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)中图像识别任务图像识别的Top-5准确率已达到97.2%，因而相关组织已经决定在2018年及以后不再每年独立举办，而是放在Kaggle上作为长期比赛，这说明现有的图像识别技术已经可以较准确地识别出图像中的对象。同时，现实社会中也出现了许多应用图像识别技术的应用，如手写文字识别、自动智能驾驶、无人便利店等。
虽然得益于CNN算法与技术不断地开发与发展，计算机视觉领域给人们展现出惊人的成果，但是由于技术、算力和数据等门槛，一般人很难拥有算力出众的机器、挑选训练数据的时间，因而一般人只能通过一些企业的演示版本来体会CNN的技术成果，而很少能够亲身体会这项技术的精准性到底能做到什么程度。
在互联网电子商务中，许多电商提供的应用程序有以图搜索商品的功能，但其在实际应用中的准确率非常低，在考虑到其搜索速度和准确率，我认为这个功能没有使用到深度学习技术或者没有充分利用，因此我认为牺牲现有端对端的以图搜索商品的便利性后，获得电商产品识别的准确率提高是具有实用意义的。

## 1.2 国内外研究现状

自1986年Rumelhart和McCelland提出了误差反向传播算法(Error back propagation, BP)后，多层前馈神经网络就从数学家的玩具成为可以为计算机所简单计算的算法。这个算法的基本原理是，输入数据经过多层前馈网络后得到一个预测值，该值与真实标记值之差称为误差，这个误差值即是预测值对于真实标记值的微分，根据BP算法，这个值可以通过链式法则，求得前馈神经网络中各个参数的微分值，经过多轮迭代后，神经网络中各个参数就会收敛于某组数值使得输入的数据经过神经网络后求得的预测值与真实标记值极小，理论上只要迭代次数足够多，该误差值就能无限接近于零。
卷积神经网络的雏形诞生于1998年，由Yann LeCun等学者提出的LeNet-5结构，该结构由5个卷积层，共7层组成，该团队成功使用这个结构，准确的识别出了手写字体，准确率比当时其他的算法都要高。这个结构的提出证明了卷积可以与前馈神经网络所结合，并且这种结合的结构具有空间不变性，比起传统的前馈神经网络，同一个层间连接的参数可以共享，节约了参数的空间和求参的时间，同时在传统的图像处理中，卷积可以提取图像的边界信息，这与当时一个生物学研究的结论相似：在视野有明显的边界变化时，生物最初的视觉神经元会有激活反应。
2006年，GE Hinton的团体提出深度置信网络(Deep belief network, DBN)，该论文指出，其一，人工神经网络所包含的隐藏层数量越多，网络的学习能力越强，越容易提取出所输入数据中内涵的特征，网络的性能越强；其二，可以通过无监督逐层初始化的方法解决人工神经网络的训练难度随着深度变深而变难的问题。在该论文的指导下，越来越多的深度神经网络结构设计被提出，是深度学习发展的一个重要里程碑
2012年，Krizhevsky等提出AlexNet模型，该团队使用该模型在ILSVRC-2012的识别任务中夺冠，准确率远超第二名，更重要的是比起上一年的冠军Top-5错误率下降了10%(绝对值)，是一个里程碑式的下降。该网络结构共13层，由5层卷积层、5层池化层和3层全连接层组成，每个卷积层都会连接一个池化层，最后3层为全连接层。该网络的设计比起LeNet-5，输入的图像尺寸从32*32变成了227*227，这里主要是池化层的成功应用的成果，此外，该设计使用的ReLu激活函数可以加快网络的训练速率，这两项成果对此后的CNN设计者有着重要的启发。
2014年，CHristian Szegedy等人设计了GoogLeNet设计，该网络的命名致敬LeNet-5，同时也是卷积神经网络的一个里程碑，它只有22层，但每层都有一个Inception的结构，即同时包含1*1，3*3，5*5，7*7的卷积，让机器自行去学习使用最适合的卷积尺寸，是一种参数稀疏的结构，因而虽然层数比AlexNet要深，参数却是其1/12；并且该结构不仅在网络的最后一层有输出层，而且在中间层中也有输出，用于避免过拟合。
2015年，Kaiming He等人提出残差卷积神经网(ResNet)的结构，将传统的浅层神经网络称为“平面网络”而其团队设计的为“残差网络”，ResNet引入了残差的概念，使得深层模型具有避免梯度消失的问题，加快模型的训练速度。该设计启发了卷积网络的设计深度不断加深。
落地应用方面，无人驾驶技术正在不断进步，2016年谷歌无人驾驶技术能够保证车辆可以自动智能穿行沙漠地区，在城市中也出现了许多载有安全测试员的无人测试车。Amazon的Amazon GO无人便利店于2018年初向公众开放，其应用了包含深度学习在内的先进技术，给顾客带来即拿即走、快速便捷的购物体验。国内阿里、京东等企业也有类似的无人便利店应用，当前主要是供内部员工使用并测试。

## 1.3 本文工作

卷积神经网络技术发展至今，对图像的识别性能已经达到类似于人类甚至超越人类的能力，

## 1.4 论文结构

# 第二章 相关原理和核心技术

# 第三章 需求分析

# 第四章 一键建模系统设计

# 第五章 总结